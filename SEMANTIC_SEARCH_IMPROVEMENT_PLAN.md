# Semantic Search Improvement Plan

## Problem Statement
Many results have `semantic_score = 0.00`, indicating semantic search is not finding matches. This reduces the effectiveness of multi-modal search.

## Root Cause Analysis

### 1. **Semantic Search Architecture** (agent/segment_tree_utils.py)
- **Embedding Model**: `all-MiniLM-L6-v2` (384 dimensions)
- **Search Targets**: 
  - Audio transcriptions (from Whisper)
  - Unified descriptions (from LLaVA, combining BLIP + object detections)
- **Similarity Metric**: Cosine similarity (0-1 range)
- **Threshold Strategy**: Starts at 0.5 → 0.4 → 0.35 if no results

### 2. **Description Generation** (extractor/generators/segment_tree_generator.py, agent/prompt.py)
- **Unified Descriptions**: Generated by LLaVA using `LLAVA_UNIFIED_PROMPT`
- **Description Style**: Includes:
  - Content (objects, people, actions, location)
  - **Technical camera details** (perspective, positioning, field of view, movement)
  - Video production style
- **Granularity**: Per-second descriptions (very granular)
- **Issue**: Technical details may not align with user queries about content

### 3. **Query Generation** (agent/planner.py, agent/prompts.py)
- **Process**: LLM generates semantic queries based on:
  - User query
  - Video inspection (keywords, objects, sample descriptions)
  - Video narrative structure (intro/body/ending)
- **Query Style**: Natural language, often abstract ("find highlights featuring action and amazing outdoor adventures")
- **Issue**: Queries may be too abstract or not aligned with description vocabulary

### 4. **Score Calculation** (agent/scoring.py)
- **Semantic Score**: Directly from cosine similarity (0-1)
- **Mapping**: `semantic_map[second_idx] = max(previous_score, new_score)`
- **Issue**: If no semantic search results match a second, score remains 0.00

## Key Issues Identified

### Issue 1: Query-Description Vocabulary Mismatch
**Problem**: 
- Queries: "find highlights featuring action and amazing outdoor adventures"
- Descriptions: "First-person POV camera mounted on backpack, wide-angle lens showing person holding fish, natural outdoor lighting, shaky camera movement"

**Root Cause**: 
- Descriptions focus on technical camera details
- Queries focus on content/emotions
- Vocabulary mismatch reduces cosine similarity

### Issue 2: Threshold Too High
**Problem**: 
- Starting threshold: 0.5 (quite strict for all-MiniLM-L6-v2)
- Many relevant matches might be filtered out

**Root Cause**: 
- all-MiniLM-L6-v2 is a lightweight model, may not achieve high similarity scores
- Threshold chosen without considering model capabilities

### Issue 3: Abstract Queries vs Concrete Descriptions
**Problem**: 
- Queries: "amazing outdoor adventures", "memorable moments"
- Descriptions: "person fishing by lake, backpack visible, boat in background"

**Root Cause**: 
- LLM generates abstract, emotional queries
- Descriptions are concrete, factual
- Embedding model may not bridge this gap well

### Issue 4: No Query Expansion/Variation
**Problem**: 
- Single query per semantic search
- No synonyms, variations, or alternative phrasings

**Root Cause**: 
- Query generation creates 1-3 queries, but doesn't expand them
- Missing query expansion techniques

### Issue 5: Description Content vs Technical Details
**Problem**: 
- Descriptions include technical camera details (50% of content)
- User queries rarely mention camera details
- Technical content dilutes semantic relevance

**Root Cause**: 
- LLaVA prompt emphasizes technical details
- No separation between content descriptions and technical metadata

## Improvement Plan

### Phase 1: Query Generation Improvements

#### 1.1 Generate Content-Focused Queries
**Action**: Modify query generation to create queries that match description style
- Generate queries using concrete vocabulary from descriptions
- Use actual keywords from video inspection
- Create queries that match the factual style of descriptions

**Files to Modify**:
- `agent/prompts.py` - `SEARCH_QUERY_GENERATION_PROMPT`
- `agent/planner.py` - Query generation section (lines 239-358)

**Implementation**:
```python
# Add instruction to generate queries that match description vocabulary
"Generate semantic queries using concrete, factual language that matches 
how video descriptions are written. Focus on objects, actions, and locations, 
not abstract concepts or emotions."
```

#### 1.2 Query Expansion
**Action**: Expand each query into multiple variations
- Generate synonyms and alternative phrasings
- Create both abstract and concrete versions
- Include variations with/without technical terms

**Files to Modify**:
- `agent/planner.py` - After query generation, expand queries

**Implementation**:
- For each semantic query, generate 2-3 variations
- Use LLM to create synonyms/alternatives
- Or use simple keyword substitution

#### 1.3 Query-Description Alignment
**Action**: Analyze sample descriptions and generate queries that match their style
- Extract common patterns from descriptions
- Generate queries using similar vocabulary
- Match query structure to description structure

**Files to Modify**:
- `agent/planner.py` - Use content_inspection sample descriptions to guide query style

### Phase 2: Threshold & Model Improvements

#### 2.1 Lower Initial Threshold
**Action**: Reduce starting threshold from 0.5 to 0.3
- all-MiniLM-L6-v2 typically produces similarities in 0.3-0.7 range
- 0.5 is too strict for this model

**Files to Modify**:
- `agent/planner.py` - Line 572: `threshold = 0.5` → `threshold = 0.3`

#### 2.2 Adaptive Threshold
**Action**: Use adaptive threshold based on result distribution
- If many results above 0.3, use 0.3
- If few results, lower to 0.25 or 0.2
- Analyze score distribution to choose optimal threshold

**Files to Modify**:
- `agent/planner.py` - Semantic search section

#### 2.3 Consider Better Embedding Model (Future)
**Action**: Evaluate larger models for better semantic understanding
- all-MiniLM-L6-v2 is fast but may lack semantic depth
- Consider: all-mpnet-base-v2, sentence-transformers/all-MiniLM-L12-v2
- Trade-off: Speed vs accuracy

### Phase 3: Description Processing

#### 3.1 Separate Content from Technical Details
**Action**: Create content-only descriptions for semantic search
- Keep technical details for other uses
- Use content-focused descriptions for embeddings
- Or weight content descriptions higher

**Files to Modify**:
- `agent/segment_tree_utils.py` - `_compute_embeddings()` method
- Option: Create two embedding sets (content-only, full)

**Implementation**:
- Extract content portion from unified descriptions
- Or modify LLaVA prompt to separate content/technical sections
- Or post-process descriptions to remove technical details

#### 3.2 Description Enhancement
**Action**: Enhance descriptions with query-relevant information
- Add action verbs, emotions, scene context
- But keep it factual and concrete

**Files to Modify**:
- `agent/prompt.py` - LLaVA prompt
- Or post-process descriptions

### Phase 4: Score Calculation Improvements

#### 4.1 Multi-Query Aggregation
**Action**: Better aggregate scores from multiple queries
- Currently: `max(previous_score, new_score)`
- Better: Weighted average or sum of top N queries
- Or: Use best match per query type

**Files to Modify**:
- `agent/scoring.py` - `score_seconds()` function, semantic_map building

#### 4.2 Query-Specific Scoring
**Action**: Track which queries matched and weight accordingly
- Some queries may be more important
- Weight matches from primary query higher
- Consider query confidence/relevance

**Files to Modify**:
- `agent/scoring.py` - Store query metadata with scores

### Phase 5: Validation & Testing

#### 5.1 Semantic Score Analysis
**Action**: Add logging to analyze semantic score distribution
- Log: average semantic scores, score ranges, match rates
- Identify: which queries work, which don't
- Track: threshold effectiveness

**Files to Modify**:
- `agent/planner.py` - Add detailed logging
- `agent/scoring.py` - Log score statistics

#### 5.2 Query-Description Matching Analysis
**Action**: Analyze why queries don't match descriptions
- Sample: Top queries vs top descriptions
- Compute: Similarity scores for analysis
- Identify: Vocabulary gaps

**Files to Modify**:
- Add analysis function to compare queries and descriptions

## Implementation Priority

### High Priority (Immediate Impact)
1. **Lower threshold** (0.5 → 0.3) - Quick fix, immediate improvement
2. **Query generation improvements** - Better alignment with descriptions
3. **Query expansion** - More coverage

### Medium Priority (Significant Impact)
4. **Content-focused descriptions** - Better semantic matching
5. **Multi-query aggregation** - Better score calculation
6. **Adaptive threshold** - Optimal threshold selection

### Low Priority (Future Enhancements)
7. **Better embedding model** - Requires performance testing
8. **Description enhancement** - May require prompt changes
9. **Advanced query analysis** - For optimization

## Expected Outcomes

### Before Fixes
- Semantic scores: Many 0.00, few above 0.3
- Match rate: Low (10-20% of seconds)
- Query effectiveness: Poor alignment

### After Fixes
- Semantic scores: More scores in 0.2-0.6 range
- Match rate: Higher (40-60% of seconds for relevant queries)
- Query effectiveness: Better alignment with descriptions

## Metrics to Track
1. **Semantic score distribution**: Average, median, percentiles
2. **Match rate**: % of seconds with semantic_score > 0
3. **Score range**: Min, max, std dev
4. **Query effectiveness**: Which queries produce most matches
5. **Threshold effectiveness**: Optimal threshold per query type

## Files to Modify

1. `agent/planner.py` - Query generation, threshold, semantic search execution
2. `agent/prompts.py` - Query generation prompt
3. `agent/scoring.py` - Score calculation, aggregation
4. `agent/segment_tree_utils.py` - Embedding computation (if separating content)
5. `agent/prompt.py` - LLaVA prompt (if modifying description style)

## Testing Strategy

1. **Unit Tests**: Test query generation, threshold logic
2. **Integration Tests**: Test semantic search with sample queries
3. **Analysis**: Run on existing video, compare before/after scores
4. **Validation**: Check that improvements don't reduce precision

